import pyodbc
import boto3
import time
from collections import OrderedDict
from logging import getLogger
import os

# =================================================================================

# ALL REQUIRED INPUTS ESSENTIAL TO RUN THE SCRIPT
# RDS

RDS_PRIVATE_HOST_ADDRESS = os.environ.get("RDS_PRIVATE_HOST_ADDRESS")
RDS_DATABASE_NAMES = os.environ.get("RDS_DATABASE_NAMES").split(",")
RDS_GLUE_CONNECTION_STR = os.environ.get("RDS_GLUE_CONNECTION_STR")
CRAWLER_OUTPUT_DB_NAME = os.environ.get("CRAWLER_OUTPUT_DB_NAME")
S3_ATHENA_OUTPUT_BUCKET_NAME = os.environ.get("S3_ATHENA_OUTPUT_BUCKET_NAME")
S3_CSV_SOURCE_BUCKET_NAME = os.environ.get("S3_CSV_SOURCE_BUCKET_NAME")
REGION = os.environ.get("AWS_REGION")

# Required to run Athena queries.
S3_RESULT_OUTPUT_LOCATION = f"s3://{S3_ATHENA_OUTPUT_BUCKET_NAME}/queries/"


S3_CSV_CATALOG_DB_NAMES = {db: f"athena_{db.lower()}" for db in RDS_DATABASE_NAMES}

# Global Objects ==================================================================
GLUE_CLIENT = boto3.client("glue", region_name=REGION)
S3_CLIENT = boto3.client("s3")
ATHENA_CLIENT = boto3.client("athena")

LOGGER = getLogger(__name__)
# =================================================================================


# F1 - Function to fetch existing Glue-RDS-MSSQLServer-DB-Connection-String
# Uses 'BOTO3'/glue library.
def get_glue_jdbc_connection(conn_name):

    response = GLUE_CLIENT.get_connection(Name=conn_name)

    connection_properties = response["Connection"]["ConnectionProperties"]
    URL = connection_properties["JDBC_CONNECTION_URL"]
    url_list = URL.split("/")

    db_host = RDS_PRIVATE_HOST_ADDRESS
    db_port = "{}".format(url_list[-1].split(":")[1])
    db_user = "{}".format(connection_properties["USERNAME"])
    db_password = "{}".format(connection_properties["PASSWORD"])

    return (
        f"DRIVER=ODBC Driver 17 for SQL Server;"
        f"SERVER={db_host},{db_port};"
        f"UID={db_user};"
        f"PWD={db_password};"
    )


# =================================================================================


# F2 - Function to fetch the user created database names using the connection string generated by 'F1'
# Uses 'PYODBC' library.
def get_rds_database_names(glue_rds_connection_string):
    db_names_str = ",".join([f"'{db}'" for db in RDS_DATABASE_NAMES])
    sql_statement = f"""
    SELECT name
    FROM sys.databases
    WHERE name in ({db_names_str}) -- NOT IN ('master', 'tempdb', 'model', 'msdb', 'experimentation', 'rdsadmin')
    ORDER BY physical_database_name;  
    """.strip()

    try:
        temp_db_list = list()
        pyodbc_conn = pyodbc.connect(
            get_glue_jdbc_connection(glue_rds_connection_string)
        )
        cursor = pyodbc_conn.cursor()

        cursor.execute(sql_statement)

        rows = cursor.fetchall()

        for row in rows:
            temp_db_list.append(row[0])

    except Exception as e:
        LOGGER.error(e)
    finally:
        cursor.close()
        pyodbc_conn.close()

    return temp_db_list


# ==================================================================================================


# F3 - Function to fetch database, schema, columns details for all the tables using the connection string generated by 'F1'
# Uses 'PYODBC' library.
def get_rds_tbl_col_order(glue_rds_connection_string, in_db_names_list):
    sql_statement = """
    SELECT TABLE_CATALOG, TABLE_SCHEMA, TABLE_NAME, COLUMN_NAME, DATA_TYPE, IS_NULLABLE, ORDINAL_POSITION
    FROM INFORMATION_SCHEMA.COLUMNS
    ORDER BY TABLE_CATALOG, TABLE_SCHEMA, TABLE_NAME, ORDINAL_POSITION
    """.strip()

    rds_db_tabl_col_order = list()
    try:
        pyodbc_conn = pyodbc.connect(
            get_glue_jdbc_connection(glue_rds_connection_string)
        )
        cursor = pyodbc_conn.cursor()

        for db in in_db_names_list:

            cursor.execute(f"use {db}")
            cursor.execute(sql_statement)

            tables_info = cursor.fetchall()

            for row in tables_info:
                rds_db_tabl_col_order.append(
                    row[0]
                    + "_"
                    + row[1]
                    + "_"
                    + row[2]
                    + "."
                    + row[3]
                    + "."
                    + str(row[6])
                )

    except Exception as e:
        LOGGER.error(e)

    finally:
        cursor.close()
        pyodbc_conn.close()

    rds_schema_dict = OrderedDict()
    rds_previous_db_tbl = ""
    for rds_full_tablename in rds_db_tabl_col_order:
        rds_table_list = rds_full_tablename.split(".")
        rds_full_db_table_name = rds_table_list[0]
        rds_column_name = rds_table_list[1]
        rds_column_order = rds_table_list[2]
        if rds_previous_db_tbl == "" or (rds_previous_db_tbl != rds_full_db_table_name):
            rds_schema_dict[rds_full_db_table_name] = dict()
            rds_schema_dict[rds_full_db_table_name].update(
                {int(rds_column_order): rds_column_name}
            )
            rds_previous_db_tbl = rds_full_db_table_name
        elif rds_previous_db_tbl == rds_full_db_table_name:
            rds_schema_dict[rds_full_db_table_name].update(
                {int(rds_column_order): rds_column_name}
            )
            rds_previous_db_tbl = rds_full_db_table_name

    return rds_schema_dict


# ==================================================================================================


# F4 - Function to fetch  column name, column type for all the tablesin Glue DataCatalog.
# Uses 'BOTO3'/glue library.
def get_gcatalog_rds_crawler_db_info(glue_catalog_db_name):
    # Define the database to run the query against
    default_database = glue_catalog_db_name

    # If you have a huge list of tables in database then you need to use nextToken to get the list of tables.
    next_token = ""
    crawler_rds_tables = list()

    while True:
        response = GLUE_CLIENT.get_tables(
            DatabaseName=default_database, NextToken=next_token
        )
        for tables in response["TableList"]:
            for columns in tables["StorageDescriptor"]["Columns"]:
                crawler_rds_tables.append(
                    tables["Name"] + "." + columns["Name"] + "." + columns["Type"]
                )
        next_token = response.get("NextToken")
        if next_token is None:
            break
    # LOGGER.info("\n", crawler_rds_tables, "\n")

    gctl_schema_dict = OrderedDict()
    previous_db_tbl = ""
    for catalog_table in crawler_rds_tables:
        catalog_table_list = catalog_table.split(".")
        database_table_name = catalog_table_list[0]
        column_name = catalog_table_list[1]
        column_type = catalog_table_list[2]
        if previous_db_tbl == "" or (previous_db_tbl != database_table_name):
            gctl_schema_dict[database_table_name] = dict()
            gctl_schema_dict[database_table_name].update({column_name: column_type})
            previous_db_tbl = database_table_name
        elif database_table_name == previous_db_tbl:
            gctl_schema_dict[database_table_name].update({column_name: column_type})
            previous_db_tbl = database_table_name
    return gctl_schema_dict


# ==================================================================================================


# F5 - Function to check if there exists s3-folder path to corresponding rds-db-table migrated.
# Uses 'OrderedDict' library.
def check_s3_path_if_exists(in_bucket_name, in_folder_path):
    result = S3_CLIENT.list_objects(Bucket=in_bucket_name, Prefix=in_folder_path)
    exists = False
    if "Contents" in result:
        exists = True
    return exists


# F6 - Function to construct s3-path for each table-folder from given source py-dictionary generated by F3.
# Uses 'OrderedDict' library.
def get_s3_csv_tbls_path(in_dict_to_be_unfolded):
    tbl_path_dict = OrderedDict()
    dir_path_str = ""
    for k in in_dict_to_be_unfolded.keys():
        dir_path_str = f"{k.split('_dbo_')[0]}/dbo/{k.split('_dbo_')[1]}"
        if check_s3_path_if_exists(S3_CSV_SOURCE_BUCKET_NAME, dir_path_str):
            full_tb_path = f"s3://{S3_CSV_SOURCE_BUCKET_NAME}/{dir_path_str}/"
            tbl_path_dict[k] = full_tb_path
    return tbl_path_dict


# ==================================================================================================


# F7 - Function to run athena create-database DDL query.
# Uses 'BOTO3'/athena library.
def create_database(in_database_name, in_s3_folder_path):
    database_ddl_str = f"""
    CREATE DATABASE IF NOT EXISTS {in_database_name}
    COMMENT 'Copy of RDS-MS-SQLServer database migrated to AWS-S3'
    LOCATION '{in_s3_folder_path}'
    WITH DBPROPERTIES (
        'owner'='Data engineering: dataengineering@digital.justice.gov.uk', 
        'environment-name'='electronic-monitoring-data-development',
        'application'='electronic-monitoring-data',
        'business-unit'='HMPPS'
    )
    """.strip()

    response = ATHENA_CLIENT.start_query_execution(
        QueryString=database_ddl_str,
        ResultConfiguration={"OutputLocation": S3_RESULT_OUTPUT_LOCATION},
    )
    return response["QueryExecutionId"]


# F8 Prepare list of athena-extrnal-table-columns and the corresponding types in string format
def get_columns_str(table_name_key, rds_dict, gctlg_dict):
    temp_list = list()
    for k, v in rds_dict[table_name_key].items():
        temp_list.append(
            f"`{v.lower()}` {gctlg_dict[table_name_key.lower()][v.lower()]}"
        )

    return ",\n".join(temp_list)


# F9 - Function to construct Athena external table DDL string using F5, F4, F3 outputs.
def build_ext_tbl_ddl_string(
    in_athena_db_name, in_table_name, in_column_list, in_location
):
    return f"""
    CREATE EXTERNAL TABLE IF NOT EXISTS {in_athena_db_name}.{in_table_name} (
        {in_column_list}
    ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
    LOCATION '{in_location}'
    TBLPROPERTIES ('classification'='csv')
    """.strip()


# F10 - Function to run athena create-external-table DDL query.
# Uses 'BOTO3'/athena library.
def create_table(table_ddl):
    response = ATHENA_CLIENT.start_query_execution(
        QueryString=table_ddl,
        ResultConfiguration={"OutputLocation": S3_RESULT_OUTPUT_LOCATION},
    )
    return response["QueryExecutionId"]


# F11 - Function to check the status of the submitted athena query
# Uses 'BOTO3'/athena library.
def has_query_succeeded(execution_id):
    state = "RUNNING"
    max_execution = 5

    while max_execution > 0 and state in ["RUNNING", "QUEUED"]:
        max_execution -= 1
        response = ATHENA_CLIENT.get_query_execution(QueryExecutionId=execution_id)
        if (
            "QueryExecution" in response
            and "Status" in response["QueryExecution"]
            and "State" in response["QueryExecution"]["Status"]
        ):
            state = response["QueryExecution"]["Status"]["State"]
            if state == "SUCCEEDED":
                return True

        time.sleep(30)

    return False


# ==================================================================================================


def handler(event, context):
    # -- 1 Fetch the given list of databases to be processed after confirming their existance in list format.
    db_names_list = get_rds_database_names(RDS_GLUE_CONNECTION_STR)
    # LOGGER.info(db_names_list)

    # -- 2 Get the details like the database-table names-&-column names alongside the column order values from RDS-MSSQLServer-DB in dict format
    rds_db_sch_tbl_col_dict = get_rds_tbl_col_order(
        RDS_GLUE_CONNECTION_STR, db_names_list
    )
    # LOGGER.info(rds_db_sch_tbl_col_dict)

    # for k, v in rds_db_sch_tbl_col_dict.items():
    #     LOGGER.info(k, v)

    # -- 3 Get the details like the database-table names-&-column names alongside the column types from AWS-Glue-DataCatalog in dict format
    gctlg_db_sch_tbl_col_dict = get_gcatalog_rds_crawler_db_info(CRAWLER_OUTPUT_DB_NAME)
    # LOGGER.info(gctl_db_sch_tbl_col_dict)

    # for k, v in gctl_db_sch_tbl_col_dict.items():
    #     LOGGER.info(k, v)

    # -- 4 Capture the existing s3 folder paths for each corresponding rds-database-table migrated.
    full_tbl_path_dict = get_s3_csv_tbls_path(rds_db_sch_tbl_col_dict)
    # LOGGER.info(full_tbl_path_dict)

    # for k, v in full_tbl_path_dict.items():
    #     LOGGER.info(k, v)

    # -- 5 Create Athena-Database(s) in AWSDataCatalog for each corresponding database exists in RDS-MSSQLServer-DB-Instance
    # Note: Drop the athena databases manually if needs to be replaced.
    for rds_db_name, athena_db_name in S3_CSV_CATALOG_DB_NAMES.items():

        if check_s3_path_if_exists(S3_CSV_SOURCE_BUCKET_NAME, f"{rds_db_name}/"):

            execution_id = create_database(
                athena_db_name, f"s3://{S3_CSV_SOURCE_BUCKET_NAME}/{rds_db_name}/"
            )
            LOGGER.info(f"Create Database execution id: {execution_id}")

            query_status = has_query_succeeded(execution_id=execution_id)
            LOGGER.info(f"Query state: {query_status}")

    # -- 6 Create Athena-External-Table if not exists already (Note: Drop the athena tables manually if needs to be replaced)
    for table_key, location in full_tbl_path_dict.items():

        if table_key.lower() in gctlg_db_sch_tbl_col_dict:
            tbl_colmns_str = get_columns_str(
                table_key, rds_db_sch_tbl_col_dict, gctlg_db_sch_tbl_col_dict
            )
            table_ddl = build_ext_tbl_ddl_string(
                S3_CSV_CATALOG_DB_NAMES[table_key.split("_dbo_")[0]],
                table_key,
                tbl_colmns_str,
                full_tbl_path_dict[table_key],
            )
            # LOGGER.info(table_ddl)
            # LOGGER.info("\n")

            # Create Table
            execution_id = create_table(table_ddl)
            LOGGER.info(f"Create Table execution id: {execution_id}")

            # Check query execution
            query_status = has_query_succeeded(execution_id=execution_id)
            LOGGER.info(f"Query state: {query_status}")
        else:
            LOGGER.warn(
                f"\n'{table_key}' - Location / Schema info not captured. Skipping ..."
            )
