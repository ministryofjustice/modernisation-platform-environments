diff --git a/terraform/environments/laa-pui-secure-browser/lambda/log_shipper/README.md b/terraform/environments/laa-pui-secure-browser/lambda/log_shipper/README.md
new file mode 100644
index 000000000..095caea36
--- /dev/null
+++ b/terraform/environments/laa-pui-secure-browser/lambda/log_shipper/README.md
@@ -0,0 +1,74 @@
+# Firewall Log Shipper
+AWS Lambda function that reads logs written to S3 (gzip or plain text, one JSON event per line, e.g., AWS Network Firewall EVE JSON) and ships them to CloudWatch Logs.
+
+## Concept of operation
+- **Trigger sources:** Handles direct **S3 ObjectCreated** events, or events delivered via **SQS** (raw S3 event in `body`) and **SNS→SQS** (S3 event wrapped in `Message`).
+- **Read & decode:** Streams the S3 object; auto-detects gzip (`ContentEncoding=gzip` or `.gz` suffix) and yields UTF-8 lines.
+- **Timestamp extraction:** For each line (JSON):
+  - Prefer `event_timestamp` (epoch seconds; string or number), then
+  - Fallback to `event.timestamp` (ISO-8601; supports `Z` or `+0000`).
+- **Event filtering:** Drops events older than **14 days** (`MAX_AGE_MS`) or more than **2 hours** in the future (`MAX_FUTURE_MS`).
+- **CloudWatch Logs write:**
+  - Uses **existing** log group (does not create it); log stream name is derived from the S3 **object key** (trimmed to 512 chars).
+  - Sorts by timestamp and ships in batches (`CHUNK_SIZE`, default 1000).
+  - Handles `InvalidSequenceTokenException` with a single refresh/retry of the upload sequence token.
+- **Return value:** `{"status": "ok"}` when a batch completes.
+
+### Environment variables
+- `LOG_GROUP_NAME` (required): Target CloudWatch Logs log group.
+
+### Tunables (constants in code)
+- `CHUNK_SIZE` (default **1000**)
+- `MAX_AGE_MS` (default **14 days**)
+- `MAX_FUTURE_MS` (default **2 hours**)
+
+## Requirements
+- **Python:** 3.11 (AWS Lambda runtime recommended)  
+- **AWS permissions (minimum):**
+  - `logs:CreateLogStream` (for the target log group)
+  - `logs:DescribeLogStreams`
+  - `logs:PutLogEvents`
+  - `s3:GetObject` (for the source bucket/prefix)
+  - If the bucket is KMS-encrypted: `kms:Decrypt` for the key
+- **Python packages:** `boto3` (available in Lambda by default; include in local/dev environments)
+
+## Installation / use
+1. **Create/choose** a CloudWatch Logs **log group** and set `LOG_GROUP_NAME` on the Lambda.
+2. **Deploy** the Lambda (runtime Python 3.11). Attach an IAM role with the permissions above.
+3. **Configure triggers:**
+   - **Direct S3:** Add an ObjectCreated notification on the bucket/prefix.
+   - **SQS:** Point S3 (or SNS) to an SQS queue; subscribe the Lambda to the queue.
+   - **SNS→SQS:** S3 → SNS topic → SQS queue → Lambda (the code unwraps the `Message`).
+4. **Test:** Upload a sample gz or text file with one JSON event per line and confirm events appear in the log group.
+
+## Command line interface (if applicable)
+No CLI is provided; the entry point is the Lambda `handler(event, context)`.
+
+## Arguments (if applicable)
+N/A. The function consumes AWS event payloads. Configuration is via environment variables and code constants.
+
+## Usage examples
+- **Direct S3 event:** Upload `s3://my-bucket/network-firewall/2025/10/15/part-0001.log.gz`. The Lambda reads, parses, filters, and writes events to the log group specified by `LOG_GROUP_NAME`, creating a log stream derived from the object key.
+- **SQS-wrapped event:** Same upload, but S3 sends the event to SQS. The Lambda unwraps `body` → `Records` → `s3`.
+
+## Operational scenarios
+- **Network Firewall centralised logging:** Ship EVE JSON from S3 to CloudWatch for dashboards, Insights queries, and alarms.
+- **Backfill/redrive:** Push historical object keys to an SQS queue; the Lambda will process in batches.
+- **Large files:** The function streams and batches; adjust memory/timeout and `CHUNK_SIZE` if needed.
+
+## Troubleshooting
+- **Nothing appears in CloudWatch Logs:**
+  - Ensure `LOG_GROUP_NAME` is correct and the log group exists (the function does **not** create it).
+  - Verify IAM allows `logs:*` actions listed above and `s3:GetObject` on the source path.
+- **`InvalidSequenceTokenException`:** The function auto-refreshes the token once. Repeated failures may indicate concurrent writers to the same stream—ensure a unique stream per object/key or serialize processing.
+- **`DataAlreadyAcceptedException` / duplicates:** Indicates overlapping retries with identical batches; safe to ignore if infrequent.
+- **`AccessDenied` / `KMS.AccessDenied`:** Add `kms:Decrypt` for the bucket’s CMK.
+- **Timestamps rejected:** Events older than 14 days or >2h future are dropped by design; check time sources and parsing.
+- **Gzip decode issues:** Ensure the object has the correct encoding or `.gz` suffix.
+
+## Safety, Auditability, and Change Control
+- **Least privilege:** The function only needs read on S3 and write to the specific log group.
+- **Deterministic ingest:** Sorted by event time; drops out-of-bounds events to respect CloudWatch constraints.
+- **No log-group creation:** Prevents accidental sprawl; manage log groups and retention via IaC (Terraform/CloudFormation).
+- **Change control:** Version the Lambda (aliases), track changes in VCS, and roll out via CI/CD with approvals.
+- **Observability:** Use CloudWatch metrics/alarms on Lambda errors, throttles, and duration; enable log retention on the target log group.
diff --git a/terraform/environments/laa-pui-secure-browser/lambda/log_shipper/log_shipper.py b/terraform/environments/laa-pui-secure-browser/lambda/log_shipper/log_shipper.py
new file mode 100644
index 000000000..16051a8be
--- /dev/null
+++ b/terraform/environments/laa-pui-secure-browser/lambda/log_shipper/log_shipper.py
@@ -0,0 +1,171 @@
+import os
+import json
+import gzip
+import io
+import time
+import datetime
+from urllib.parse import unquote_plus
+
+import boto3
+
+logs = boto3.client("logs")
+s3 = boto3.client("s3")
+
+LOG_GROUP = os.environ["LOG_GROUP_NAME"]
+CHUNK_SIZE = 1000  # conservative; lower if log lines can be large
+MAX_AGE_MS = 14 * 24 * 3600 * 1000
+MAX_FUTURE_MS = 2 * 3600 * 1000
+
+
+def handler(event, context):
+    now_ms = int(time.time() * 1000)
+
+    for rec in _iter_s3_records(event):
+        bucket = rec["s3"]["bucket"]["name"]
+        key = unquote_plus(rec["s3"]["object"]["key"])
+        stream_name = key[-512:]  # ensure <= 512 chars
+
+        # 1) Read the object (gz or plain)
+        obj = s3.get_object(Bucket=bucket, Key=key)
+        body = obj["Body"]
+        is_gz = (obj.get("ContentEncoding", "").lower() == "gzip") or key.endswith(".gz")
+
+        # Build events: [{"timestamp": <ms>, "message": <line>}, ...]
+        events = []
+        for line in _iter_lines(body, gz=is_gz):
+            line = line.strip()
+            if not line:
+                continue
+
+            ts_ms = _extract_ts_ms(line)
+            if ts_ms is None:
+                continue  # skip lines without a usable timestamp
+
+            # Respect CloudWatch Logs time bounds
+            if ts_ms < (now_ms - MAX_AGE_MS) or ts_ms > (now_ms + MAX_FUTURE_MS):
+                continue
+
+            events.append({"timestamp": ts_ms, "message": line})
+
+        if not events:
+            continue
+
+        events.sort(key=lambda e: e["timestamp"])
+
+        # 2) Ensure stream exists (no log group creation here—keep it minimal)
+        try:
+            logs.create_log_stream(logGroupName=LOG_GROUP, logStreamName=stream_name)
+        except logs.exceptions.ResourceAlreadyExistsException:
+            pass
+
+        # 3) Ship in chunks, with one simple sequence-token retry path
+        seq = _get_upload_seq_token(LOG_GROUP, stream_name)
+        i = 0
+        while i < len(events):
+            batch = events[i : i + CHUNK_SIZE]
+            args = {
+                "logGroupName": LOG_GROUP,
+                "logStreamName": stream_name,
+                "logEvents": batch,
+            }
+            if seq:
+                args["sequenceToken"] = seq
+            try:
+                resp = logs.put_log_events(**args)
+                seq = resp.get("nextSequenceToken")
+                i += len(batch)
+            except logs.exceptions.InvalidSequenceTokenException:
+                seq = _get_upload_seq_token(LOG_GROUP, stream_name)  # refresh once
+
+    return {"status": "ok"}
+
+
+def _iter_s3_records(event):
+    """
+    Yield S3 event records from:
+      - Direct S3 event (rec contains 's3')
+      - SQS with raw S3 body (rec['body'] has {"Records":[...]})
+      - SQS with SNS envelope (rec['body'] has {"Message":"{...Records...}"})
+    """
+    for rec in event.get("Records", ()):
+        # Already an S3 event (e.g., direct S3 invoke)
+        if "s3" in rec:
+            yield rec
+            continue
+
+        # Likely SQS → body is JSON
+        body = rec.get("body")
+        if not body:
+            continue
+        try:
+            outer = json.loads(body)
+        except Exception:
+            continue
+
+        # SNS-wrapped? (Message is a JSON string of the original S3 event)
+        if isinstance(outer, dict) and "Message" in outer and isinstance(outer["Message"], str):
+            try:
+                inner = json.loads(outer["Message"])
+            except Exception:
+                continue
+        else:
+            inner = outer
+
+        for s3rec in inner.get("Records", []) or []:
+            if "s3" in s3rec:
+                yield s3rec
+
+
+def _iter_lines(body, gz: bool):
+    """Yield UTF-8 lines from an S3 StreamingBody, supporting gzip."""
+    if gz:
+        # TextIOWrapper gives us str lines directly with the right decoding.
+        with gzip.GzipFile(fileobj=body) as raw:
+            for line in io.TextIOWrapper(raw, encoding="utf-8", errors="replace"):
+                yield line
+    else:
+        for b in body.iter_lines(chunk_size=64 * 1024):
+            yield b.decode("utf-8", errors="replace")
+
+
+def _extract_ts_ms(line: str):
+    """Return event time in epoch milliseconds from Network Firewall EVE JSON."""
+    try:
+        obj = json.loads(line)
+    except json.JSONDecodeError:
+        return None
+
+    # Prefer epoch seconds in top-level event_timestamp (often a string)
+    et = obj.get("event_timestamp")
+    if et is not None:
+        try:
+            return int(float(et) * 1000)  # preserve fractional seconds
+        except (TypeError, ValueError):
+            pass
+
+    # Fallback to ISO8601 inside event.timestamp, e.g. 2025-09-29T17:20:03.799527+0000 or ...Z
+    ts = (obj.get("event") or {}).get("timestamp")
+    if ts:
+        # Normalize 'Z' to '+0000' so strptime yields an aware datetime
+        if ts.endswith("Z"):
+            ts = ts[:-1] + "+0000"
+        for fmt in ("%Y-%m-%dT%H:%M:%S.%f%z", "%Y-%m-%dT%H:%M:%S%z"):
+            try:
+                dt = datetime.datetime.strptime(ts, fmt)
+                return int(dt.timestamp() * 1000)
+            except ValueError:
+                continue
+    return None
+
+
+def _get_upload_seq_token(log_group, stream_name):
+    """Return current uploadSequenceToken (or None) for the stream."""
+    resp = logs.describe_log_streams(
+        logGroupName=log_group,
+        logStreamNamePrefix=stream_name,
+        limit=1,
+    )
+    streams = resp.get("logStreams") or []
+    if streams and streams[0]["logStreamName"] == stream_name:
+        return streams[0].get("uploadSequenceToken")
+    return None
diff --git a/terraform/environments/laa-pui-secure-browser/lambda/log_shipper/requirements.txt b/terraform/environments/laa-pui-secure-browser/lambda/log_shipper/requirements.txt
new file mode 100644
index 000000000..5813ced5c
--- /dev/null
+++ b/terraform/environments/laa-pui-secure-browser/lambda/log_shipper/requirements.txt
@@ -0,0 +1 @@
+boto3~=1.34
\ No newline at end of file
diff --git a/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/cloudwatch.tf b/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/cloudwatch.tf
index 5533d31d7..9a4f0f424 100644
--- a/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/cloudwatch.tf
+++ b/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/cloudwatch.tf
@@ -139,3 +139,10 @@ resource "aws_cloudwatch_dashboard" "workspacesweb_active_sessions" {
     ]
   })
 }
+
+resource "aws_cloudwatch_log_group" "workspacesweb_session_logs" {
+  count             = local.create_resources ? 1 : 0
+  kms_key_id        = aws_kms_key.workspacesweb_session_logs[0].id
+  name_prefix       = "laa-workspacesweb-session-logs-"
+  retention_in_days = 14
+}
\ No newline at end of file
diff --git a/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/data.tf b/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/data.tf
index 1048620da..8c3b153d6 100644
--- a/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/data.tf
+++ b/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/data.tf
@@ -1,3 +1,9 @@
+data "archive_file" "lambda" {
+  type        = "zip"
+  source_file = "../lambda/log_shipper/log_shipper.py"
+  output_path = "${path.module}/.build/log_shipper.zip"
+}
+
 # Look up availability zones to map zone IDs to names
 data "aws_availability_zones" "available" {
   state = "available"
diff --git a/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/iam.tf b/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/iam.tf
index 308381afd..73293e73c 100644
--- a/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/iam.tf
+++ b/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/iam.tf
@@ -37,7 +37,7 @@ module "cortex_xsiam_role" {
         "sqs:GetQueueUrl",
         "sqs:ListQueues",
       ]
-      resources = [module.sqs_s3_notifications[0].queue_arn]
+      resources = [module.sqs_xsiam_notifications[0].queue_arn]
     }
     S3GetLogs = {
       effect = "Allow"
diff --git a/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/lambda.tf b/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/lambda.tf
new file mode 100644
index 000000000..dbb1e4cc0
--- /dev/null
+++ b/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/lambda.tf
@@ -0,0 +1,61 @@
+# trivy:ignore:AVD-AWS-0066
+module "lambda_s3_log_processor" {
+  count   = local.create_resources ? 1 : 0
+  source  = "terraform-aws-modules/lambda/aws"
+  version = "~> 8.0"
+
+  function_name = "s3-log-processor"
+  handler       = "log_shipper.handler"
+  runtime       = "python3.12"
+  timeout       = 900
+  memory_size   = 512
+
+  create_role              = true
+  attach_policies          = true
+  attach_policy_statements = true
+  number_of_policies       = 1
+  policies                 = ["arn:aws:iam::aws:policy/service-role/AWSLambdaSQSQueueExecutionRole"]
+  policy_statements = {
+    kms_decrypt_s3 = {
+      effect    = "Allow"
+      actions   = ["kms:Decrypt"]
+      resources = [aws_kms_key.workspacesweb_session_logs[0].arn]
+    }
+    logs_write = {
+      effect    = "Allow"
+      actions   = ["logs:CreateLogStream", "logs:DescribeLogStreams", "logs:PutLogEvents"]
+      resources = [aws_cloudwatch_log_group.workspacesweb_session_logs[0].arn, "${aws_cloudwatch_log_group.workspacesweb_session_logs[0].arn}:log-stream:*"]
+    }
+    sqs_read = {
+      effect    = "Allow"
+      actions   = ["sqs:ReceiveMessage", "sqs:DeleteMessage", "sqs:GetQueueAttributes", "sqs:GetQueueUrl", "sqs:ChangeMessageVisibility"]
+      resources = [module.sqs_lambda_consumer[0].queue_arn]
+    }
+    s3_read = {
+      effect    = "Allow"
+      actions   = ["s3:GetObject"]
+      resources = ["${module.s3_bucket_workspacesweb_session_logs[0].s3_bucket_arn}/firewall/AWSLogs/*"]
+    }
+  }
+
+  create_package         = false
+  local_existing_package = data.archive_file.lambda.output_path
+
+  # Environment
+  environment_variables = {
+    LOG_GROUP_NAME = aws_cloudwatch_log_group.workspacesweb_session_logs[0].name
+  }
+
+  event_source_mapping = {
+    s3_events_from_sqs = {
+      event_source_arn                   = module.sqs_lambda_consumer[0].queue_arn
+      enabled                            = true
+      batch_size                         = 10
+      maximum_batching_window_in_seconds = 5
+      function_response_types            = ["ReportBatchItemFailures"]
+      scaling_config = {
+        maximum_concurrency = 10
+      }
+    }
+  }
+}
\ No newline at end of file
diff --git a/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/s3.tf b/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/s3.tf
index a80e935a6..2b791f432 100644
--- a/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/s3.tf
+++ b/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/s3.tf
@@ -65,15 +65,21 @@ resource "random_string" "bucket_suffix" {
   upper   = false
 }
 
+# Because we can't send notifications to two destinations we need to fan-out via an SNS topic
 resource "aws_s3_bucket_notification" "s3_bucket_workspacesweb_session_logs" {
   count  = local.create_resources ? 1 : 0
   bucket = module.s3_bucket_workspacesweb_session_logs[0].s3_bucket_id
   queue {
     id            = "workspaces-web-logs"
-    queue_arn     = module.sqs_s3_notifications[0].queue_arn
+    queue_arn     = module.sqs_xsiam_notifications[0].queue_arn
     events        = ["s3:ObjectCreated:*"]
     filter_prefix = "workspaces-web-logs/"
   }
+  topic {
+    events        = ["s3:ObjectCreated:*"]
+    filter_prefix = "workspaces-web-logs/"
+    topic_arn     = module.s3_workspacesweb_session_logs_sns_topic.topic_arn
+  }
 }
 
 data "aws_iam_policy_document" "s3_bucket_policy" {
diff --git a/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/sns.tf b/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/sns.tf
new file mode 100644
index 000000000..939ad506b
--- /dev/null
+++ b/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/sns.tf
@@ -0,0 +1,34 @@
+module "s3_workspacesweb_session_logs_sns_topic" {
+  source  = "terraform-aws-modules/sns/aws"
+  version = "6.2.0"
+
+  name = "s3-firewall-log-notifications"
+  subscriptions = {
+    cortex = {
+      protocol = "sqs"
+      endpoint = module.sqs_xsiam_notifications[0].queue_arn
+    }
+    lambda_consumer = {
+      protocol = "sqs"
+      endpoint = module.sqs_lambda_consumer[0].queue_arn
+    }
+  }
+  topic_policy_statements = {
+    allow_s3_publish = {
+      actions    = ["sns:Publish"]
+      principals = [{ type = "Service", identifiers = ["s3.amazonaws.com"] }]
+      conditions = [
+        { test = "ArnLike", variable = "aws:SourceArn", values = [module.s3_bucket_workspacesweb_session_logs[0].s3_bucket_arn] },
+        { test = "StringEquals", variable = "aws:SourceAccount", values = [data.aws_caller_identity.current.account_id] }
+      ]
+    }
+    allow_sqs_subscribe = {
+      actions    = ["sns:Subscribe"]
+      principals = [{ type = "AWS", identifiers = [data.aws_caller_identity.current.account_id] }]
+      conditions = [
+        { test = "StringEquals", variable = "sns:Protocol", values = ["sqs"] },
+        { test = "StringEquals", variable = "sns:Endpoint", values = [module.sqs_xsiam_notifications] }
+      ]
+    }
+  }
+}
diff --git a/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/sqs.tf b/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/sqs.tf
index d319ffdb4..d40561549 100644
--- a/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/sqs.tf
+++ b/terraform/environments/laa-pui-secure-browser/workspaces-secure-browser/sqs.tf
@@ -1,4 +1,4 @@
-module "sqs_s3_notifications" {
+module "sqs_xsiam_notifications" {
   source = "terraform-aws-modules/sqs/aws"
   #checkov:skip=CKV_TF_1:Module registry does not support commit hashes for versions
   version = "5.1.0"
@@ -29,3 +29,33 @@ module "sqs_s3_notifications" {
     }
   }
 }
+
+module "sqs_lambda_consumer" {
+  source = "terraform-aws-modules/sqs/aws"
+  #checkov:skip=CKV_TF_1:Module registry does not support commit hashes for versions
+  version             = "5.1.0"
+  count               = local.create_resources ? 1 : 0
+  create_dlq          = true
+  create_queue_policy = true
+  name                = "${local.component_name}-lambda-consumer"
+  queue_policy_statements = {
+    sns_publish = {
+      sid        = "AllowSNSTopicSendMessage"
+      actions    = ["sqs:SendMessage"]
+      principals = [{ type = "Service", identifiers = ["sns.amazonaws.com"] }]
+      conditions = [
+        { test = "ArnEquals", variable = "aws:SourceArn", values = [module.s3_workspacesweb_session_logs_sns_topic.topic_arn] },
+        { test = "StringEquals", variable = "aws:SourceAccount", values = [data.aws_caller_identity.current.account_id] }
+      ]
+    }
+  }
+  redrive_policy = {
+    maxReceiveCount = 5
+  }
+  visibility_timeout_seconds = 5400
+}
+
+moved {
+  from = module.sqs_s3_notifications
+  to   = module.sqs_xsiam_notifications
+}
